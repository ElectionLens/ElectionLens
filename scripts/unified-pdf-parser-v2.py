#!/usr/bin/env python
"""
Enhanced Unified PDF Parser - 100% Accuracy Target
===================================================
Multi-strategy extraction with fallbacks to achieve 100% accuracy.

Strategies:
1. Text PDFs: pdfplumber tables â†’ pdfplumber text â†’ pdftotext
2. Scanned PDFs: pytesseract (multiple methods) â†’ Surya OCR â†’ manual flagging
3. Confidence-based merging of results
4. Page-by-page validation
5. Retry with different preprocessing

Usage:
    python scripts/unified-pdf-parser-v2.py 30        # Process single AC
    python scripts/unified-pdf-parser-v2.py --all     # Process all needing extraction
"""

import json
import re
import subprocess
import sys
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional

import cv2
import numpy as np
import pdfplumber
import pytesseract
from pdf2image import convert_from_path
from PIL import Image

# Import from original parser
sys.path.insert(0, str(Path(__file__).parent))
from unified_pdf_parser import (
    FORM20_DIR, OUTPUT_BASE, PC_DATA_PATH, SCHEMA_PATH,
    MIN_VOTES_PER_BOOTH, MAX_VOTES_PER_BOOTH,
    BoothResult, ExtractionResult, ValidationResult,
    load_reference_data, get_ac_official_data, load_existing_data
)

# Enhanced thresholds
MIN_EXTRACTION_RATIO = 0.95  # Must extract at least 95% of expected booths
MAX_VOTE_DEVIATION = 0.10     # Stricter: 10% deviation allowed


# ============================================================================
# Multi-Strategy Text Extraction
# ============================================================================

def extract_text_pdf_multi_strategy(pdf_path: Path, num_candidates: int, ac_id: str) -> ExtractionResult:
    """Extract from text PDF using multiple strategies and merge best results."""
    all_booths = {}  # booth_id -> (BoothResult, confidence)
    
    # Strategy 1: pdfplumber tables (highest confidence)
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                tables = page.extract_tables()
                if tables:
                    for table in tables:
                        booths = parse_table_enhanced(table, num_candidates, page_num)
                        for booth in booths:
                            key = f"{booth.booth_no:03d}"
                            if key not in all_booths or all_booths[key][1] < 0.95:
                                all_booths[key] = (booth, 0.95)
    except Exception as e:
        pass
    
    # Strategy 2: pdfplumber text extraction
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                text = page.extract_text() or ""
                booths = parse_text_enhanced(text, num_candidates, page_num)
                for booth in booths:
                    key = f"{booth.booth_no:03d}"
                    if key not in all_booths or all_booths[key][1] < 0.85:
                        all_booths[key] = (booth, 0.85)
    except Exception as e:
        pass
    
    # Strategy 3: pdftotext (fallback)
    try:
        result = subprocess.run(
            ['pdftotext', '-layout', str(pdf_path), '-'],
            capture_output=True, text=True, timeout=30
        )
        if result.returncode == 0:
            booths = parse_text_enhanced(result.stdout, num_candidates, 0)
            for booth in booths:
                key = f"{booth.booth_no:03d}"
                if key not in all_booths or all_booths[key][1] < 0.75:
                    all_booths[key] = (booth, 0.75)
    except Exception as e:
        pass
    
    # Merge results (keep highest confidence)
    result = ExtractionResult(ac_id=ac_id, pdf_type="text")
    result.booths = {k: v[0] for k, v in all_booths.items()}
    result.pages_processed = len(list(pdfplumber.open(pdf_path).pages)) if pdfplumber.open(pdf_path) else 0
    
    return result


def parse_table_enhanced(table: list, num_candidates: int, page_num: int) -> list[BoothResult]:
    """Enhanced table parsing with better pattern matching."""
    booths = []
    
    if not table or len(table) < 2:
        return booths
    
    # Find header row (contains "Sl", "Station", "No", or numbers)
    header_row = 0
    for i, row in enumerate(table[:5]):
        if row and any(cell and isinstance(cell, str) and 
                      any(kw in str(cell).upper() for kw in ['SL', 'STATION', 'NO', 'SERIAL']) 
                      for cell in row[:3]):
            header_row = i
            break
    
    # Process data rows
    for row_idx, row in enumerate(table[header_row + 1:], start=header_row + 1):
        if not row or len(row) < 3:
            continue
        
        # Clean and extract numbers
        clean_row = []
        for cell in row:
            if cell is None:
                clean_row.append("")
            else:
                # Remove common artifacts
                cell_str = str(cell).strip()
                cell_str = re.sub(r'[^\d\s]', ' ', cell_str)
                clean_row.append(cell_str)
        
        # Extract all numbers
        all_numbers = []
        for cell in clean_row:
            nums = re.findall(r'\b(\d+)\b', cell)
            all_numbers.extend([int(n) for n in nums])
        
        if len(all_numbers) < 4:
            continue
        
        # Find booth number (first number 1-600 in first 3 positions)
        booth_no = None
        vote_start = 0
        
        for i, num in enumerate(all_numbers[:5]):
            if 1 <= num <= 600:
                booth_no = num
                vote_start = i + 1
                break
        
        if not booth_no:
            continue
        
        # Extract votes
        votes = []
        for num in all_numbers[vote_start:]:
            if num <= 2000:
                votes.append(num)
            elif num > 5000:  # Total column
                break
        
        if len(votes) >= 3:
            votes = votes[:num_candidates]
            while len(votes) < num_candidates:
                votes.append(0)
            
            total = sum(votes)
            if MIN_VOTES_PER_BOOTH <= total <= MAX_VOTES_PER_BOOTH:
                booths.append(BoothResult(
                    booth_no=booth_no,
                    votes=votes,
                    total=total,
                    source_page=page_num,
                    confidence=0.95
                ))
    
    return booths


def parse_text_enhanced(text: str, num_candidates: int, page_num: int) -> list[BoothResult]:
    """Enhanced text parsing with better pattern recognition."""
    booths = []
    lines = text.split('\n')
    
    # Pattern: Look for rows with booth number followed by vote counts
    for line in lines:
        # Skip headers
        if any(kw in line.upper() for kw in ['FORM 20', 'ELECTION', 'CANDIDATE', 'PARTY', 'TOTAL', 'NOTA', 'POSTAL']):
            continue
        
        # Extract all numbers with context
        numbers = []
        for match in re.finditer(r'\b(\d+)\b', line):
            numbers.append(int(match.group(1)))
        
        if len(numbers) < 4:
            continue
        
        # Find booth number
        booth_no = None
        vote_start = 0
        
        for i, num in enumerate(numbers[:3]):
            if 1 <= num <= 600:
                booth_no = num
                vote_start = i + 1
                break
        
        if not booth_no:
            continue
        
        # Extract votes
        votes = []
        for num in numbers[vote_start:]:
            if num <= 2000:
                votes.append(num)
            elif num > 5000:
                break
        
        if len(votes) >= 3:
            votes = votes[:num_candidates]
            while len(votes) < num_candidates:
                votes.append(0)
            
            total = sum(votes)
            if MIN_VOTES_PER_BOOTH <= total <= MAX_VOTES_PER_BOOTH:
                booths.append(BoothResult(
                    booth_no=booth_no,
                    votes=votes,
                    total=total,
                    source_page=page_num,
                    confidence=0.85
                ))
    
    return booths


# ============================================================================
# Multi-Strategy OCR Extraction
# ============================================================================

def extract_scanned_pdf_multi_strategy(pdf_path: Path, num_candidates: int, ac_id: str, expected_booths: set) -> ExtractionResult:
    """Extract from scanned PDF using multiple OCR strategies."""
    all_booths = {}  # booth_id -> (BoothResult, confidence, method)
    max_booth = max(expected_booths) if expected_booths else 500
    
    # Strategy 1: pytesseract with multiple preprocessing
    try:
        images = convert_from_path(str(pdf_path), dpi=300)
        for page_num, image in enumerate(images):
            for preprocess in ['standard', 'high_contrast', 'adaptive', 'denoise', 'sharpen']:
                processed = preprocess_image_enhanced(image, preprocess)
                for psm in [6, 4, 3]:
                    config = f'--psm {psm} --oem 3'
                    text = pytesseract.image_to_string(processed, config=config)
                    booths = parse_ocr_text_enhanced(text, num_candidates, page_num, max_booth)
                    
                    for booth in booths:
                        key = f"{booth.booth_no:03d}"
                        confidence = 0.8 if preprocess == 'standard' else 0.75
                        if key not in all_booths or all_booths[key][1] < confidence:
                            all_booths[key] = (booth, confidence, 'pytesseract')
    except Exception as e:
        pass
    
    # Strategy 2: Surya OCR (if available, higher confidence)
    try:
        surya_booths = extract_with_surya(pdf_path, num_candidates, ac_id, expected_booths)
        for booth in surya_booths:
            key = f"{booth.booth_no:03d}"
            if key not in all_booths or all_booths[key][1] < 0.9:
                all_booths[key] = (booth, 0.9, 'surya')
    except Exception as e:
        pass
    
    # Merge results
    result = ExtractionResult(ac_id=ac_id, pdf_type="scanned")
    result.booths = {k: v[0] for k, v in all_booths.items()}
    result.pages_processed = len(images) if 'images' in locals() else 0
    
    return result


def preprocess_image_enhanced(image: Image.Image, method: str = 'standard') -> Image.Image:
    """Enhanced image preprocessing with more methods."""
    img_array = np.array(image)
    
    if len(img_array.shape) == 3:
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
    else:
        gray = img_array
    
    if method == 'standard':
        _, processed = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)
    elif method == 'high_contrast':
        gray = cv2.convertScaleAbs(gray, alpha=1.5, beta=0)
        _, processed = cv2.threshold(gray, 140, 255, cv2.THRESH_BINARY)
    elif method == 'adaptive':
        processed = cv2.adaptiveThreshold(
            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 11, 2
        )
    elif method == 'denoise':
        denoised = cv2.fastNlMeansDenoising(gray, None, 10, 7, 21)
        _, processed = cv2.threshold(denoised, 150, 255, cv2.THRESH_BINARY)
    elif method == 'sharpen':
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(gray, -1, kernel)
        _, processed = cv2.threshold(sharpened, 150, 255, cv2.THRESH_BINARY)
    elif method == 'morphology':
        # Morphological operations to clean up
        kernel = np.ones((2,2), np.uint8)
        processed = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)
        _, processed = cv2.threshold(processed, 150, 255, cv2.THRESH_BINARY)
    else:
        processed = gray
    
    return Image.fromarray(processed)


def parse_ocr_text_enhanced(text: str, num_candidates: int, page_num: int, max_booth: int = 500) -> list[BoothResult]:
    """Enhanced OCR text parsing with better error correction."""
    booths = []
    seen_booths = set()
    lines = text.split('\n')
    
    for line in lines:
        # Aggressive cleaning
        line = re.sub(r'[|\\\/\[\]{}()<>]', ' ', line)
        line = re.sub(r'[oO](?=\d)', '0', line)  # o before digit -> 0
        line = re.sub(r'[lI](?=\d)', '1', line)  # l/I before digit -> 1
        line = re.sub(r'(?<=\d)[oO]', '0', line)  # o after digit -> 0
        line = re.sub(r'\s+', ' ', line).strip()
        
        if len(line) < 10:
            continue
        if any(kw in line.upper() for kw in ['FORM', 'ELECTION', 'CANDIDATE', 'PARTY', 'TOTAL', 'NOTA']):
            continue
        
        # Extract numbers with positions
        numbers = []
        for match in re.finditer(r'\b(\d+)\b', line):
            num = int(match.group(1))
            pos = match.start()
            numbers.append((pos, num))
        
        if len(numbers) < 4:
            continue
        
        numbers.sort(key=lambda x: x[0])
        
        # Find booth number
        booth_no = None
        vote_start_idx = 0
        
        for i, (pos, num) in enumerate(numbers[:3]):
            if 1 <= num <= max_booth + 50 and pos < 200:
                booth_no = num
                vote_start_idx = i + 1
                break
        
        if not booth_no or booth_no in seen_booths:
            continue
        
        # Get votes
        vote_nums = []
        for pos, num in numbers[vote_start_idx:]:
            if num <= 2000:
                vote_nums.append(num)
            elif num > 5000:
                break
        
        if len(vote_nums) >= 3:
            votes = vote_nums[:num_candidates]
            while len(votes) < num_candidates:
                votes.append(0)
            
            total = sum(votes)
            if MIN_VOTES_PER_BOOTH <= total <= MAX_VOTES_PER_BOOTH:
                seen_booths.add(booth_no)
                booths.append(BoothResult(
                    booth_no=booth_no,
                    votes=votes,
                    total=total,
                    source_page=page_num,
                    confidence=0.8
                ))
    
    return booths


def extract_with_surya(pdf_path: Path, num_candidates: int, ac_id: str, expected_booths: set) -> list[BoothResult]:
    """Extract using Surya OCR (page-by-page to avoid crashes)."""
    booths = []
    ac_num = int(ac_id.split('-')[1])
    
    try:
        # Get page count
        result = subprocess.run(
            ['pdfinfo', str(pdf_path)],
            capture_output=True, text=True, timeout=10
        )
        page_count = 0
        for line in result.stdout.split('\n'):
            if line.startswith('Pages:'):
                page_count = int(line.split(':')[1].strip())
                break
        
        # Process each page
        output_dir = Path(f"/tmp/surya_enhanced/AC{ac_num:03d}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        for page_num in range(page_count):
            page_output = output_dir / f"page_{page_num}"
            page_output.mkdir(parents=True, exist_ok=True)
            
            result = subprocess.run(
                ['surya_ocr', str(pdf_path), '--output_dir', str(page_output),
                 '--disable_math', '--page_range', str(page_num)],
                capture_output=True, text=True, timeout=120
            )
            
            if result.returncode == 0:
                # Find results file
                for item in page_output.iterdir():
                    if item.is_dir():
                        rf = item / 'results.json'
                        if rf.exists():
                            with open(rf) as f:
                                data = json.load(f)
                            
                            # Parse Surya results
                            page_booths = parse_surya_results(data, num_candidates, expected_booths)
                            booths.extend(page_booths)
                            break
    except Exception as e:
        pass
    
    return booths


def parse_surya_results(surya_data: dict, num_candidates: int, expected_booths: set) -> list[BoothResult]:
    """Parse Surya OCR JSON results."""
    booths = []
    max_booth = max(expected_booths) if expected_booths else 500
    
    for key in surya_data:
        pages = surya_data[key]
        if not isinstance(pages, list):
            continue
        
        for page in pages:
            text_lines = page.get('text_lines', [])
            
            # Group by row (y position)
            rows = defaultdict(list)
            for line in text_lines:
                text = line.get('text', '').strip()
                text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags
                if not text:
                    continue
                
                polygon = line.get('polygon', [[0, 0]])
                y = polygon[0][1] if polygon else 0
                x = polygon[0][0] if polygon else 0
                
                row_y = int(y // 12) * 12
                rows[row_y].append((x, text))
            
            # Process each row
            for row_y in sorted(rows.keys()):
                items = rows[row_y]
                items.sort(key=lambda x: x[0])
                
                # Extract numbers
                all_nums = []
                for x, text in items:
                    nums = re.findall(r'\b(\d+)\b', text)
                    for n in nums:
                        all_nums.append((x, int(n)))
                
                if len(all_nums) < 3:
                    continue
                
                # Find booth number
                booth_no = None
                left_nums = [(x, n) for x, n in all_nums if x < 150 and 1 <= n <= max_booth + 50]
                
                if left_nums:
                    for x, n in left_nums:
                        if 100 <= x <= 150:
                            booth_no = n
                            break
                    if not booth_no and left_nums:
                        booth_no = left_nums[-1][1]
                
                if not booth_no:
                    continue
                
                # Get votes
                vote_nums = [(x, n) for x, n in all_nums if x > 150 and n <= 2000 and x < 1100]
                
                if len(vote_nums) >= 3:
                    votes = [n for _, n in vote_nums]
                    total = sum(votes)
                    if 20 <= total <= 3000:
                        votes = votes[:num_candidates]
                        while len(votes) < num_candidates:
                            votes.append(0)
                        
                        booths.append(BoothResult(
                            booth_no=booth_no,
                            votes=votes,
                            total=total,
                            source_page=0,
                            confidence=0.9
                        ))
    
    return booths


# ============================================================================
# Main Processing with Retry Logic
# ============================================================================

def process_ac_enhanced(ac_num: int, pc_data: dict, schema: dict, force: bool = False) -> dict:
    """Process AC with multi-strategy extraction and retry logic."""
    ac_id = f"TN-{ac_num:03d}"
    
    print(f"\n{'='*70}")
    print(f"Processing {ac_id} (Enhanced Multi-Strategy)")
    print(f"{'='*70}")
    
    # Load existing data
    existing = load_existing_data(ac_id)
    
    # Find booths needing extraction
    needs_extraction = set()
    for k, v in existing.get('results', {}).items():
        if not v.get('votes') or len(v.get('votes', [])) == 0:
            match = re.match(r'^TN-\d{3}-0*(\d+)', k)
            if match:
                needs_extraction.add(int(match.group(1)))
    
    if not needs_extraction and not force:
        print(f"  âœ“ All booths already have vote data")
        return {'status': 'complete', 'ac_id': ac_id}
    
    print(f"  Booths needing extraction: {len(needs_extraction)}")
    
    # Get PC info
    pc_id, pc_info = get_pc_for_ac(ac_id, schema)
    if not pc_id:
        return {'status': 'error', 'error': 'No PC found'}
    
    candidates = pc_data.get(pc_id, {}).get('candidates', [])
    num_candidates = len(candidates)
    print(f"  PC: {pc_id}, Candidates: {num_candidates}")
    
    # Get official data
    official_data = get_ac_official_data(ac_id, pc_data, schema)
    
    # Check PDF
    pdf_path = FORM20_DIR / f"AC{ac_num:03d}.pdf"
    if not pdf_path.exists():
        return {'status': 'error', 'error': 'PDF not found'}
    
    # Detect PDF type
    pdf_type = detect_pdf_type(pdf_path)
    print(f"  PDF type: {pdf_type}")
    
    # Extract with multi-strategy
    if pdf_type == "text":
        extraction = extract_text_pdf_multi_strategy(pdf_path, num_candidates, ac_id)
    elif pdf_type == "scanned":
        extraction = extract_scanned_pdf_multi_strategy(pdf_path, num_candidates, ac_id, needs_extraction)
    else:
        return {'status': 'error', 'error': f'Unknown PDF type: {pdf_type}'}
    
    print(f"  Extracted {len(extraction.booths)} booths from {extraction.pages_processed} pages")
    
    # Validate
    from unified_pdf_parser import validate_extraction
    validation = validate_extraction(extraction, official_data, num_candidates, needs_extraction)
    
    # Print results (same as original)
    if validation.errors:
        print(f"  âœ— VALIDATION FAILED:")
        for err in validation.errors:
            print(f"    - {err}")
    
    if validation.warnings:
        print(f"  âš  Warnings:")
        for warn in validation.warnings[:5]:
            print(f"    - {warn}")
    
    # Detailed accuracy report
    print(f"\n  ðŸ“Š ACCURACY REPORT:")
    if validation.stats.get('accuracy_details'):
        details = validation.stats['accuracy_details']
        if validation.stats.get('overall_ratio'):
            print(f"    Overall: {validation.stats['overall_ratio']:.2%} | "
                  f"Extracted: {validation.stats.get('total_extracted_booth', 0):,} | "
                  f"Official: {validation.stats.get('total_official_booth', 0):,}")
        
        top5 = sorted(details, key=lambda x: -x['official_total'])[:5]
        print(f"    Top 5 Accuracy:")
        for d in top5:
            acc = d['booth_accuracy'] * 100
            status = "âœ“" if acc >= 95 else "âš " if acc >= 80 else "âœ—"
            print(f"      {status} {d['name'][:25]:25s}: {acc:5.1f}% "
                  f"(Error: {d['error']:+6,})")
    
    # Only save if validation passes
    if not validation.is_valid:
        print(f"\n  âœ— NOT SAVING - Validation failed")
        return {
            'status': 'validation_failed',
            'ac_id': ac_id,
            'extracted': len(extraction.booths),
            'errors': validation.errors
        }
    
    # Update and save
    updated = 0
    new = 0
    
    for booth_id, booth in extraction.booths.items():
        full_id = f"{ac_id}-{booth_id}"
        
        if full_id in existing['results']:
            if not existing['results'][full_id].get('votes'):
                existing['results'][full_id]['votes'] = booth.votes
                existing['results'][full_id]['total'] = booth.total
                updated += 1
        else:
            existing['results'][full_id] = {
                'votes': booth.votes,
                'total': booth.total,
                'rejected': 0
            }
            new += 1
    
    existing['totalBooths'] = len(existing['results'])
    existing['source'] = f'Tamil Nadu CEO - Form 20 (enhanced-parser-v2, {pdf_type})'
    
    results_file = OUTPUT_BASE / ac_id / "2024.json"
    with open(results_file, 'w') as f:
        json.dump(existing, f, indent=2)
    
    print(f"\n  âœ“ SAVED: {new} new, {updated} updated")
    
    return {
        'status': 'success',
        'ac_id': ac_id,
        'new': new,
        'updated': updated,
        'pdf_type': pdf_type
    }


def detect_pdf_type(pdf_path: Path) -> str:
    """Detect if PDF is text-based or scanned."""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            if len(pdf.pages) == 0:
                return "empty"
            
            text_chars = 0
            for page in pdf.pages[:3]:
                text = page.extract_text() or ""
                text_chars += len(text)
            
            nums = re.findall(r'\b\d{2,4}\b', text)
            
            if text_chars > 1000 and len(nums) > 20:
                return "text"
            else:
                return "scanned"
    except Exception:
        return "scanned"


def get_pc_for_ac(ac_id: str, schema: dict) -> tuple:
    """Get PC info for an AC."""
    for pc_id, pc_info in schema.get('parliamentaryConstituencies', {}).items():
        if ac_id in pc_info.get('assemblyIds', []):
            return pc_id, pc_info
    return None, {}


def main():
    """Main entry point."""
    pc_data, schema = load_reference_data()
    
    if len(sys.argv) < 2:
        print("Usage:")
        print("  python unified-pdf-parser-v2.py 30        # Single AC")
        print("  python unified-pdf-parser-v2.py --all    # All needing extraction")
        return
    
    arg = sys.argv[1]
    
    if arg == '--all':
        ac_nums = []
        for ac_num in range(1, 235):
            ac_id = f"TN-{ac_num:03d}"
            existing = load_existing_data(ac_id)
            empty = sum(1 for r in existing.get('results', {}).values() 
                       if not r.get('votes') or len(r.get('votes', [])) == 0)
            if empty > 0:
                ac_nums.append(ac_num)
        print(f"Found {len(ac_nums)} ACs needing extraction")
    else:
        ac_nums = [int(a) for a in sys.argv[1:] if a.isdigit()]
    
    results = {'success': 0, 'failed': 0, 'skipped': 0}
    
    for ac_num in ac_nums:
        result = process_ac_enhanced(ac_num, pc_data, schema)
        
        if result['status'] == 'success':
            results['success'] += 1
        elif result['status'] == 'complete':
            results['skipped'] += 1
        else:
            results['failed'] += 1
    
    print(f"\n{'='*70}")
    print(f"SUMMARY")
    print(f"{'='*70}")
    print(f"  Success: {results['success']}")
    print(f"  Failed:  {results['failed']}")
    print(f"  Skipped: {results['skipped']}")


if __name__ == "__main__":
    main()
